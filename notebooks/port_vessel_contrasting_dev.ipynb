{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Configuration\n",
    "# ----------------------------------------\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Model Paths\n",
    "# ----------------------------------------\n",
    "BASE_MODEL_PATH = \"microsoft/deberta-v3-small\"\n",
    "BGE_MODEL_PATH = \"../models/DeBERTa/bge\"\n",
    "SBERT_MODEL_PATH = \"../models/DeBERTa/sbert\"\n",
    "SIMCSE_MODEL_PATH = \"../models/DeBERTa/simcse\"\n",
    "MLM_MODEL_PATH = \"../models/DeBERTa/mlm\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# Loading Tokenizer and Models\n",
    "# ----------------------------------------\n",
    "print(\"Loading models...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n",
    "\n",
    "base_model = AutoModel.from_pretrained(BASE_MODEL_PATH).to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "bge_model = AutoModel.from_pretrained(BGE_MODEL_PATH).to(DEVICE)\n",
    "bge_model.eval()\n",
    "\n",
    "sbert_model = AutoModel.from_pretrained(SBERT_MODEL_PATH).to(DEVICE)\n",
    "sbert_model.eval()\n",
    "\n",
    "simcse_model = AutoModel.from_pretrained(SIMCSE_MODEL_PATH).to(DEVICE)\n",
    "simcse_model.eval()\n",
    "\n",
    "mlm_model = AutoModel.from_pretrained(MLM_MODEL_PATH).to(DEVICE)\n",
    "mlm_model.eval()\n",
    "\n",
    "print(\"All models loaded successfully!\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load and Prepare the Datasets\n",
    "# ----------------------------------------\n",
    "print(\"Loading port vessel dataset...\")\n",
    "base_df = pd.read_json(\"../data/email_datasets/synthetic/attrprompting/claude/aggregated/aggregated.json\")\n",
    "base_df[\"vessel_label\"] = base_df.labels.apply(lambda x: x[\"vessel\"].lower().strip() if isinstance(x, dict) and \"vessel\" in x else \"\")\n",
    "base_df[\"lp_label\"] = base_df.labels.apply(lambda x: x[\"load_port\"].lower().strip() if isinstance(x, dict) and \"load_port\" in x else \"\")\n",
    "base_df[\"dp_label\"] = base_df.labels.apply(lambda x: x[\"discharge_port\"].lower().strip() if isinstance(x, dict) and \"discharge_port\" in x else \"\")\n",
    "\n",
    "base_df_clean = base_df[\n",
    "    (base_df[\"vessel_label\"] != \"\") & \n",
    "    (base_df[\"lp_label\"] != \"\") & \n",
    "    (base_df[\"dp_label\"] != \"\")\n",
    "].copy()\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {base_df_clean.shape}\")\n",
    "print(f\"Unique vessels: {base_df_clean['vessel_label'].nunique()}\")\n",
    "print(f\"Unique load ports: {base_df_clean['lp_label'].nunique()}\")\n",
    "print(f\"Unique discharge ports: {base_df_clean['dp_label'].nunique()}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Embedding Generation\n",
    "# ----------------------------------------\n",
    "def get_embeddings_batch(texts, model, tokenizer, batch_size=32, pooling_method='mean'):\n",
    "    \"\"\"Generate embeddings for a batch of texts\"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            if pooling_method == 'cls':\n",
    "                embeddings_batch = outputs.last_hidden_state[:, 0]\n",
    "            \n",
    "            else: \n",
    "                attention_mask = inputs['attention_mask']\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                embeddings_batch = sum_embeddings / sum_mask\n",
    "\n",
    "            embeddings.append(embeddings_batch.cpu().numpy())\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def prepare_combined_entity_dataset(df, min_frequency=2):\n",
    "    \"\"\"Prepare combined dataset of vessels and ports with category labels\"\"\"\n",
    "    \n",
    "    vessel_counts = df['vessel_label'].value_counts()\n",
    "    frequent_vessels = vessel_counts[vessel_counts >= min_frequency].index.tolist()\n",
    "    \n",
    "    all_ports = list(df['lp_label'].tolist()) + list(df['dp_label'].tolist())\n",
    "    port_counts = pd.Series(all_ports).value_counts()\n",
    "    frequent_ports = port_counts[port_counts >= min_frequency].index.tolist()\n",
    "    \n",
    "    entity_texts = frequent_vessels + frequent_ports\n",
    "    entity_categories = ['Vessel'] * len(frequent_vessels) + ['Port'] * len(frequent_ports)\n",
    "    \n",
    "    print(f\"Entity Analysis:\")\n",
    "    print(f\"Total unique vessels: {len(vessel_counts)}\")\n",
    "    print(f\"Frequent vessels (min_freq={min_frequency}): {len(frequent_vessels)}\")\n",
    "    print(f\"Total unique ports: {len(port_counts)}\")\n",
    "    print(f\"Frequent ports (min_freq={min_frequency}): {len(frequent_ports)}\")\n",
    "    print(f\"Combined entities for analysis: {len(entity_texts)}\")\n",
    "    \n",
    "    print(f\"\\nTop vessels by frequency: {dict(vessel_counts.head(10))}\")\n",
    "    print(f\"Top ports by frequency: {dict(port_counts.head(10))}\")\n",
    "    \n",
    "    return entity_texts, entity_categories\n",
    "\n",
    "def create_tsne_visualization(base_emb, bge_emb, sbert_emb, simcse_emb, mlm_emb, entity_texts, entity_categories):\n",
    "    \"\"\"Create t-SNE visualization for entity embeddings colored by category\"\"\"\n",
    "    print(f\"Computing t-SNE projections for maritime entities...\")\n",
    "    \n",
    "    perplexity = min(30, len(entity_texts) - 1)\n",
    "    if perplexity < 2:\n",
    "        print(f\"Skipping t-SNE: too few entities\")\n",
    "        return\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    \n",
    "    base_2d = tsne.fit_transform(base_emb)\n",
    "    bge_2d = tsne.fit_transform(bge_emb)\n",
    "    sbert_2d = tsne.fit_transform(sbert_emb)\n",
    "    simcse_2d = tsne.fit_transform(simcse_emb)\n",
    "    mlm_2d = tsne.fit_transform(mlm_emb)\n",
    "    \n",
    "    all_embeddings = np.vstack([base_2d, bge_2d, sbert_2d, simcse_2d, mlm_2d])\n",
    "    x_limits = all_embeddings[:, 0].min()*1.05, all_embeddings[:, 0].max()*1.05\n",
    "    y_limits = all_embeddings[:, 1].min()*1.05, all_embeddings[:, 1].max()*1.05\n",
    "    \n",
    "    # Create 2x3 subplot grid\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(24, 16), dpi=300)\n",
    "    \n",
    "    category_colors = {\n",
    "        'Vessel': '#1f77b4',\n",
    "        'Port': '#ff7f0e' \n",
    "    }\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # DEBERTa Base\n",
    "    # ----------------------------------------\n",
    "    for i, (entity_text, category) in enumerate(zip(entity_texts, entity_categories)):\n",
    "        ax1.scatter(base_2d[i, 0], base_2d[i, 1],\n",
    "                   c=category_colors[category],\n",
    "                   label=category if i == 0 or category != entity_categories[i-1] else \"\",\n",
    "                   alpha=0.7,\n",
    "                   s=50)\n",
    "\n",
    "        if len(entity_texts) <= 25:\n",
    "            ax1.annotate(entity_text[:12], (base_2d[i, 0], base_2d[i, 1]), \n",
    "                        fontsize=7, alpha=0.7)\n",
    "    \n",
    "    ax1.set_title('DeBERTa-v3-small (Base)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('t-SNE 1')\n",
    "    ax1.set_ylabel('t-SNE 2')\n",
    "    ax1.set_xlim(x_limits)\n",
    "    ax1.set_ylim(y_limits)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # DEBERTa BGE\n",
    "    # ----------------------------------------\n",
    "    for i, (entity_text, category) in enumerate(zip(entity_texts, entity_categories)):\n",
    "        ax2.scatter(bge_2d[i, 0], bge_2d[i, 1],\n",
    "                   c=category_colors[category],\n",
    "                   label=category if i == 0 or category != entity_categories[i-1] else \"\",\n",
    "                   alpha=0.7,\n",
    "                   s=50)\n",
    "        if len(entity_texts) <= 25:\n",
    "            ax2.annotate(entity_text[:12], (bge_2d[i, 0], bge_2d[i, 1]), \n",
    "                        fontsize=7, alpha=0.7)\n",
    "    \n",
    "    ax2.set_title('DeBERTa + BGE (bge-large-en-v1.5)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('t-SNE 1')\n",
    "    ax2.set_ylabel('t-SNE 2')\n",
    "    ax2.set_xlim(x_limits)\n",
    "    ax2.set_ylim(y_limits)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # DEBERTa SBERT\n",
    "    # ----------------------------------------\n",
    "    for i, (entity_text, category) in enumerate(zip(entity_texts, entity_categories)):\n",
    "        ax3.scatter(sbert_2d[i, 0], sbert_2d[i, 1],\n",
    "                   c=category_colors[category],\n",
    "                   label=category if i == 0 or category != entity_categories[i-1] else \"\",\n",
    "                   alpha=0.7,\n",
    "                   s=50)\n",
    "        if len(entity_texts) <= 25:\n",
    "            ax3.annotate(entity_text[:12], (sbert_2d[i, 0], sbert_2d[i, 1]), \n",
    "                        fontsize=7, alpha=0.7)\n",
    "    \n",
    "    ax3.set_title('DeBERTa + SBERT (all-MiniLM-L6-v2)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('t-SNE 1')\n",
    "    ax3.set_ylabel('t-SNE 2')\n",
    "    ax3.set_xlim(x_limits)\n",
    "    ax3.set_ylim(y_limits)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # DEBERTa SimCSE\n",
    "    # ----------------------------------------\n",
    "    for i, (entity_text, category) in enumerate(zip(entity_texts, entity_categories)):\n",
    "        ax4.scatter(simcse_2d[i, 0], simcse_2d[i, 1],\n",
    "                   c=category_colors[category],\n",
    "                   label=category if i == 0 or category != entity_categories[i-1] else \"\",\n",
    "                   alpha=0.7,\n",
    "                   s=50)\n",
    "        if len(entity_texts) <= 25:\n",
    "            ax4.annotate(entity_text[:12], (simcse_2d[i, 0], simcse_2d[i, 1]), \n",
    "                        fontsize=7, alpha=0.7)\n",
    "    \n",
    "    ax4.set_title('DeBERTa + SimCSE Finetuned', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('t-SNE 1')\n",
    "    ax4.set_ylabel('t-SNE 2')\n",
    "    ax4.set_xlim(x_limits)\n",
    "    ax4.set_ylim(y_limits)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # DEBERTa MLM\n",
    "    # ----------------------------------------\n",
    "    for i, (entity_text, category) in enumerate(zip(entity_texts, entity_categories)):\n",
    "        ax5.scatter(mlm_2d[i, 0], mlm_2d[i, 1],\n",
    "                   c=category_colors[category],\n",
    "                   label=category if i == 0 or category != entity_categories[i-1] else \"\",\n",
    "                   alpha=0.7,\n",
    "                   s=50)\n",
    "        if len(entity_texts) <= 25:\n",
    "            ax5.annotate(entity_text[:12], (mlm_2d[i, 0], mlm_2d[i, 1]), \n",
    "                        fontsize=7, alpha=0.7)\n",
    "    \n",
    "    ax5.set_title('DeBERTa + MLM (Maritime Domain)', fontsize=14, fontweight='bold')\n",
    "    ax5.set_xlabel('t-SNE 1')\n",
    "    ax5.set_ylabel('t-SNE 2')\n",
    "    ax5.set_xlim(x_limits)\n",
    "    ax5.set_ylim(y_limits)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.legend()\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.02)\n",
    "    \n",
    "    output_path = os.path.join(\"../output/embeddings_tsne\", \"port_vessel_embeddings_with_mlm.png\")\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Visualization saved as: {output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MARITIME ENTITY EMBEDDINGS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "entity_texts, entity_categories = prepare_combined_entity_dataset(base_df_clean, min_frequency=2)\n",
    "\n",
    "if len(entity_texts) < 4:\n",
    "    print(\"Not enough entities for analysis\")\n",
    "else:\n",
    "    print(f\"\\nGenerating embeddings for {len(entity_texts)} maritime entities...\")\n",
    "    \n",
    "    base_embeddings = get_embeddings_batch(\n",
    "        entity_texts, base_model, tokenizer, BATCH_SIZE, pooling_method='mean'\n",
    "    )\n",
    "    \n",
    "    bge_embeddings = get_embeddings_batch(\n",
    "        entity_texts, bge_model, tokenizer, BATCH_SIZE, pooling_method='cls'\n",
    "    )\n",
    "    \n",
    "    sbert_embeddings = get_embeddings_batch(\n",
    "        entity_texts, sbert_model, tokenizer, BATCH_SIZE, pooling_method='cls'\n",
    "    )\n",
    "    \n",
    "    simcse_embeddings = get_embeddings_batch(\n",
    "        entity_texts, simcse_model, tokenizer, BATCH_SIZE, pooling_method='cls'\n",
    "    )\n",
    "    \n",
    "    mlm_embeddings = get_embeddings_batch(\n",
    "        entity_texts, mlm_model, tokenizer, BATCH_SIZE, pooling_method='mean'\n",
    "    )\n",
    "    \n",
    "    print(\"Embeddings generated successfully!\")\n",
    "    \n",
    "    create_tsne_visualization(\n",
    "        base_embeddings, bge_embeddings, sbert_embeddings, simcse_embeddings, mlm_embeddings,\n",
    "        entity_texts, entity_categories\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_custom_env)",
   "language": "python",
   "name": "my_custom_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
